As a final "fun" example, let's see how to employ an $objectToArray operator expression to use reflection to analyse the shape of a collection of documents 
as part of a custom schema analysis tool. Such reflection capabilities are vital in databases that provide a flexible data model, such as MongoDB, 
where the included fields may vary from document to document.

Imagine you have a collection of customer documents, similar to the following:

```js
db.customers.insertMany([
    {
        "_id": ObjectId('6064381b7aa89666258201fd'),
        "email": 'elsie_smith@myemail.com',
        "dateOfBirth": ISODate('1991-05-30T08:35:52.000Z'),
        "accNnumber": 123456,
        "balance": NumberDecimal("9.99"),
        "address": {
        "firstLine": "1 High Street",
        "city": "Newtown",
        "postcode": "NW1 1AB",
        },
        "telNums": ["07664883721", "01027483028"],
        "optedOutOfMarketing": true,
    },
    {
        "_id": ObjectId('734947394bb73732923293ed'),
        "email": 'jon.jones@coolemail.com',
        "dateOfBirth": ISODate('1993-07-11T22:01:47.000Z'),
        "accNnumber": 567890,
        "balance": NumberDecimal("299.22"),
        "telNums": "07836226281",
        "contactPrefernece": "email",
    },
]);
```

In your schema analysis pipeline, you use $objectToArray to capture the name and type of each top-level field in the document as follows:

```js
var pipeline = [
    {"$project": {
    "_id": 0,
    "schema": {
    "$map": {
    "input": {"$objectToArray": "$$ROOT"},
    "as": "field",
    "in": {
    "fieldname": "$$field.k",
    "type": {"$type": "$$field.v"},          
    }
    }
    }
    }}
];
db.customers.aggregate(pipeline);
```

For the two example documents in the collection, the pipeline outputs the following:
```js
{
schema: [
{fieldname: '_id', type: 'objectId'},
{fieldname: 'email', type: 'string'},
{fieldname: 'dateOfBirth', type: 'date'},
{fieldname: 'accNnumber', type: 'int'},
{fieldname: 'balance', type: 'decimal'},
{fieldname: 'address', type: 'object'},
{fieldname: 'telNums', type: 'array'},
{fieldname: 'optedOutOfMarketing', type: 'bool'}
]
},
{
schema: [
{fieldname: '_id', type: 'objectId'},
{fieldname: 'email', type: 'string'},
{fieldname: 'dateOfBirth', type: 'date'},
{fieldname: 'accNnumber', type: 'int'},
{fieldname: 'balance', type: 'decimal'},
{fieldname: 'telNums', type: 'string'},
{fieldname: 'contactPrefernece', type: 'string'}
}

```


The difficulty with this basic pipeline approach is once there are many documents in the collection, the output will be too lengthy and complex for you to detect common schema 
patterns. Instead, you will want to add an $unwind and $group stage combination to accumulate recurring fields that match. 
The generated result should also highlight if the same field name appears in multiple documents but with different data types. Here is the improved pipeline:

```js
var pipeline = [
    {"$project": {
            "_id": 0,
            "schema": {
                "$map": {
                    "input": {"$objectToArray": "$$ROOT"},
                    "as": "field",
                    "in": {
                        "fieldname": "$$field.k",
                        "type": {"$type": "$$field.v"},
                    }
                }
            }
        }},

    {"$unwind": "$schema"},

    {"$group": {
            "_id": "$schema.fieldname",
            "types": {"$addToSet": "$schema.type"},
        }},

    {"$set": {
            "fieldname": "$_id",
            "_id": "$$REMOVE",
        }},
];


db.customers.aggregate(pipeline);


{fieldname: '_id', types: ['objectId']},
{fieldname: 'address', types: ['object']},
{fieldname: 'email', types: ['string']},
{fieldname: 'telNums', types: ['string', 'array']},
{fieldname: 'contactPrefernece', types: ['string']},
{fieldname: 'accNnumber', types: ['int']},
{fieldname: 'balance', types: ['decimal']},
{fieldname: 'dateOfBirth', types: ['date']},
{fieldname: 'optedOutOfMarketing', types: ['bool']}
```
This result highlights that the telNums field can have one of two different data types within documents.
The main drawback of this rudimentary schema analysis pipeline is its inability to descend through layers of arrays and sub-documents hanging off each top-level document. 
This challenge is indeed solvable using a pure aggregation pipeline, but the code involved is far more complex and beyond the scope of this chapter. 
If you are interested in exploring this further, the "mongo-agg-schema-analyzer" GitHub project solves this problem. 
That project shows you how to traverse through hierarchically structured documents using a single aggregation pipeline to infer the schema.
